---
title: "biostatistical methods homework 4"
output: pdf_document
---

```{r message=FALSE}
library(tidyverse)
library(knitr)
library(patchwork)
library(readxl)
```

# Problem1

## (a)

$$
\begin{aligned}
b_{1} &= \frac{n\sum X_{i}Y_{i}-\sum X_{i}\sum Y_{i}}{n\sum X_{i}^{2}-(\sum X_{i})^{2}} \\
&= \frac{\sum X_{i}Y_{i}-n\bar{Y}\bar{X}}{\sum X_{i}^{2}-n\bar{X}^{2}} \\
\\
b_{0} &= \bar{Y}-b_{1}\bar{X}
\\
\\
\sum X_{i}Y_{i}-n\bar{Y}\bar{X} &= \sum X_{i}Y_{i} - \bar{X}\sum{Y_{i}} \\
&= \sum (X_{i}-\bar{X})Y_{i}\\
\\
E\left \{\sum (X_{i} - \bar{X})Y_{i}  \right \} &= \sum(X_{i}-\bar{X})E(Y_{i})\\
&= \sum (X_{i}-\bar{X})(\beta_{0}+\beta_{1}X_{i})\\
&= \beta_{0}\sum X_{i} - n\bar{X}\beta_{0} + \beta_{1}\sum X_{i}^{2} - n\bar{X}^{2}\beta_{1} \\
&= \beta_{1}(\sum X_{i}^2 - n\bar{X}^2)\\
\\
E(b_{1}) &= \frac{E\left \{\sum (X_{i}-\bar{X})Y_{i}  \right \}}{\sum X_{i}^{2}-n\bar{X}^2}\\
&= \frac{\beta_{1}(\sum X_{i}^2 - n\bar{X}^2)}{\sum X_{i}^2 - n\bar{X}^2}\\
&= \beta_{1}\\
\\
E(b_{0}) &= E(\bar{Y}-b_{1}\bar{X})\\
&= \frac{1}{n}\sum E(Y_{i}) - E(b_{1})\bar{X}\\
&= \frac{1}{n}\sum \left [\beta_{0} + \beta_{1}X_{i}  \right ]-\beta_{1}\bar{X}\\
&= \frac{1}{n}\left [n\beta_{0}+n\beta_{1}\bar{X}  \right ]- \beta_{1}\bar{X}\\
&= \beta_{0}
\end{aligned}
$$

## (b)

$$
\begin{aligned}
Y_{i} &= \hat{\beta_{1}}X_{i} + \hat{\beta_{0}}\\
&= \hat{\beta_{1}}X_{i} + \bar{Y} - \hat{\beta_{1}}\bar{X}\\
\\
X_i\ &= \bar{X}\\
Y_{i} &= \hat{\beta_{1}}\bar{X} + \bar{Y} - \hat{\beta_{1}}\bar{X}\\
&=\bar{Y}
\end{aligned}
$$

So the Least Square line equation always goes through the point $(\bar{X}, \bar{Y})$

## (c)
$$
\begin{aligned}
&log_{e}L = -\frac{n}{2}log_{e}2\pi - \frac{n}{2}log_{e}\sigma^{2}-\frac{1}{2\sigma^{2}}\sum (Y_{i}-\beta_{0}-\beta_{1}X_{i})^{2}\\
&\frac{\partial(log_{e}L)}{\partial\sigma^{2}} = -\frac{n}{2\sigma^{2}} + \frac{1}{2\sigma^{4}}\sum (Y_{i}-\beta_{0}-\beta_{1}X_{i})^{2}\\
\end{aligned}
$$

$$
\begin{aligned}
\hat{\sigma}^{2}&=\frac{\sum (Y_{i}-\hat{\beta_{0}}-\hat{\beta_{1}}X_{i})^{2}}{n}\\
&=\frac{\sum(Y_{i}-\hat{Y_{i}})^{2}}{n}
\end{aligned}
$$

### Find its expected value

$$
\begin{aligned}
E(\hat{\sigma}^{2}) &= E\left (\frac{SSE}{n}  \right )\\
&=E\left (\frac{SSE}{n-2}\times \frac{n-2}{n}  \right )\\
&=\frac{n-2}{n} \times E\left (\frac{SSE}{n-2}  \right )\\
&=\frac{n-2}{n}\sigma^{2}
\end{aligned}
$$

### Comment on the unbiasness property

As the result shown above, $\hat{\sigma}^{2}$ is a biasd estimator of $\sigma^{2}$ as the unbiased estimator of $\sigma^{2}$ is MSE:

$$
\begin{aligned}
s^{2} = MSE = \frac{SSE}{n-2} &=  \frac{\sum(Y_{i}-\hat{Y_{i}})^{2}}{n-2} = \frac{\sum e_{i}^2}{n-2}\\
\\
E\left \{MSE  \right \} &= \sigma^{2}
\end{aligned}
$$

# Problem 2

For this problem, you will be using data ‘HeartDisease.csv’. 

```{r message=FALSE}
heart_data = read_csv("./data/HeartDisease.csv")
```

The investigator is mainly interested if there is an association between ‘total cost’ (in dollars) of patients diagnosed with heart disease and the ‘number of emergency room (ER) visits’. 

Further, the model will need to be adjusted for other factors, including ‘age’, ‘gender’, ‘number of complications’ that arose during treatment, and ‘duration of treatment condition’.

## a) 

Provide a short description of the data set: what is the main outcome, main predictor and other important covariates. 


This dataset include `r ncol(heart_data)` variables and `r nrow(heart_data)` observations. The main outcome is `totalcost` which represents the total cost (in dollars) of heart-diseased patients. The main predictor is the `ERvisits` which represents the number of emergency room (ER) visits. Other important covariates are `age`, `gender`, `complications` and `duration`. 


Also, generate appropriate descriptive statistics for all variables of interest (continuous and categorical) – no test required. 

```{r}
mean_and_sd = function(x) {
  
  if (!is.numeric(x)) {
    stop("Argument x should be numeric")
  } else if (length(x) == 1) {
    stop("Cannot be computed for length 1 vectors")
  }
  
  mean_x = mean(x)
  sd_x = sd(x)

  tibble(
    mean = mean_x, 
    sd = sd_x
  )
}
```

### totalcost
```{r}
mean_and_sd(heart_data$totalcost)
```

The mean of the total cost is about 2800 with a standard deviation of 6690.26.

### ERvisits
```{r}
summary(heart_data$ERvisits)
```

The minimum number of emergency room (ER) visits is 0 and the maximum is 20. The median is 3 with 1st Qu. of 2 and 3rd Qu. of 5. 

### age
```{r}
mean_and_sd(heart_data$age)
```

The distribution of age is centered at about 59 with a standard deviation of 6.75.

### gender
```{r}
(summary(as.factor(heart_data$gender)))
```

As 0 represents female and 1 represents male, there are 608 female and 180 male in the dataset.

### complications
```{r}
(summary(as.factor(heart_data$complications)))
```

As we observed from the dataset, there number of complications existing in this dataset is simply 0, 1 and 3. Using summary function, we can conclude that there are 745 patients have zero complicatoins and 42 patients have one complicatoins, and there is only 1 patient has 3 complicaton.

### duration
```{r}
mean_and_sd(heart_data$duration)
```

The average duration of treatment condition is 164 with a standard deviation of 121.


## b) 


```{r}
totalcost_non = heart_data %>%
  ggplot(aes(x = totalcost)) +
  geom_density() + 
  labs(
       x = 'Total Cost',
       y = 'Density'
       )
```

```{r}
totalcost_sq = heart_data %>%
  ggplot(aes(x = (totalcost)^2)) +
  geom_density() +
  labs(
       x = 'Square Transformation of Total Cost',
       y = 'Density'
       ) 
  
```

```{r}
totalcost_log = heart_data %>%
  ggplot(aes(x = log(totalcost))) +
  geom_density() + 
  labs(
       x = 'Log Transformation of Total Cost',
       y = 'Density'
       ) 
```

```{r}
totalcost_sqrt = heart_data %>%
  ggplot(aes(x = sqrt(totalcost))) +
  geom_density() +
  labs(
      x = 'Square Root Transformation of Total Cost',
      y = 'Density'
      )
```

```{r}
(totalcost_non + totalcost_sq) / (totalcost_log + totalcost_sqrt)
```

The shape of the distribution for `totalcost` is right skewed. After trying different transformation we find that the log transformation makes the plot approximate to normal distribution.



## c) 

Create a new variable called ‘comp_bin’ by dichotomizing ‘complications’: 0 if no complications, and 1 otherwise.

```{r}
heart_data = heart_data %>%
  mutate(comp_bin = ifelse(complications == 0, 0, 1)) %>%
  mutate(comp_bin = as.character(comp_bin))
```


## d) 

Based on our decision in part b), fit a simple linear regression (SLR) between the original or transformed ‘total cost’ and predictor ‘ERvisits’. This includes a scatterplot and results of the regression, with appropriate comments on significance and interpretation of the slope.


```{r}
heart_data_trans = heart_data 

heart_data_trans$totalcost[heart_data_trans$totalcost==0]=0.01
  
heart_data_trans = heart_data_trans %>%  
  mutate(totalcost = log(totalcost))

```


```{r}
heart_data_trans %>%
  ggplot(aes(x = ERvisits, y = totalcost)) +
  geom_point(color = 'blue') +
  geom_smooth(method = "lm", color = 'red', se = FALSE) +
  labs(
      x = 'The number of ER visits',
      y = 'Log Transformation of Total Cost'
      )
```


```{r}
fit_SLR = lm(totalcost ~ ERvisits, data = heart_data_trans)
summary(fit_SLR)
```


The plot above shows the scatterplot and results of the regression. Using `summary` function, we can see that the estimate slope is 0.2251 with a p-value <2e-16, which strongly indicates that the slope is not equal to 0 and there is significant relationship with `ERvisits` and `totalcost`. The estimate of slope means that when the number of ER visits increase 1, total cost will increase 25%.


$$
\begin{aligned}
log\left (\frac{Y_{2}}{Y_{1}}  \right ) &= \beta_{1} = 0.22672\\
\frac{Y_{2}}{Y_{1}} &= exp^{0.2251} = 1.25\\
Y_{2} &= 1.25Y_{1}
\end{aligned}
$$

## e) 

Fit a multiple linear regression (MLR) with ‘comp_bin’ and ‘ERvisits’ as predictors.


### i) 

Test if ‘comp_bin’ is an effect modifier of the relationship between ‘total cost’ and ‘ERvisits’. Comment. 

```{r}
fit_MLR_interaction = lm(totalcost ~ comp_bin * ERvisits, data = heart_data_trans)
summary(fit_MLR_interaction)

```

The definition of modifier is when the magnitude of association differs at different levels of another variable (in this case `comp_bin`), it suggests that effect modification is present. From the result shown above, `comp_bin` is not a modifier according to the p-value of comp_bin1:ERvisits is way larger than 0.05.

### ii)

Test if ‘comp_bin’ is a confounder of the relationship between ‘total cost’ and ‘ERvisits’.
Comment. 

```{r}
lm(totalcost ~ ERvisits, data = heart_data_trans) %>%
  summary()

#coefficient estimate: 
#ERvisits: 0.2251


lm(totalcost ~ comp_bin + ERvisits, data = heart_data_trans) %>%
  summary()

#coefficient estimate: 
#ERvisits: 0.20236

```

To calculate the percentage change in the parameter estimate, we use the following formula:

$$
\frac{\left |\beta_{crude}-\beta_{adjusted}  \right |}{\left |\beta_{crude}  \right |}=\frac{\left |0.2251-0.20236  \right |}{\left |0.2251  \right |}=0.1010218
$$
Here we use 10% rule of thumb. 0.1010218 is greater than 10%, so we consider `comp_bin` as a confounder.

### iii) 

Decide if ‘comp_bin’ should be included along with ‘ERvisits'. Why or why not?

Hypothese:

$$
\begin{aligned}
&Model\ 1:\ Y_{i}=\beta_{0} \ +\ \beta_{1}X_{i\ ERvisits} \ +\  \varepsilon_{i}\\
&Model\ 2:\ Y_{i}=\beta_{0} \ +\ \beta_{1}X_{i\ ERvisits} \ +\ \beta_{2}X_{i\ comp\_bin} \ +\  \varepsilon_{i}\\
\end{aligned}
$$

$$
\begin{aligned}
&H_{0} : \beta_{2} = 0\\
&H_{1} : \beta_{2} \neq 0
\end{aligned}
$$
Decision rule:

$$
F^{*} = \frac{(SSE_{S}-SSE_{L})/(df_{L}-df_{S})}{\frac{SSE_{L}}{df_{L}}} \sim F_{df_{L}-df_{S},\ df_{L}}
$$
$$
df_{S}=n-p_{S}-1,\ df_{L}=n-p_{L}-1
$$

$$
\begin{aligned}
&F^{*}=34.694\\
\end{aligned}
$$

$$
\begin{aligned}
&Pr(F>F^{*})< 0.05,\ reject\ H_{0}\\
&Pr(F>F^{*})\geq 0.05,\ fail\ to\ reject\ H_{0}
\end{aligned}
$$

$$
\begin{aligned}
&Pr(F>F^{*})=5.721e-09< 0.05,\ reject\ H_{0}\\
\end{aligned}
$$

```{r}
fit_without_comp = lm(totalcost ~ ERvisits, data = heart_data_trans)
fit_with_comp = lm(totalcost ~ ERvisits + comp_bin, data = heart_data_trans)

anova(fit_without_comp, fit_with_comp)

```

According to the result of partial F-test, the larger model including `comp_bin` is prefered. Besides, we already proof that `comp_bin` is a confounder so it should be included in the model along with ERvisits.

## f) 

Use your choice of model in part e) and add additional covariates (age, gender, and duration of treatment).

### i) 

Fit a MLR, show the regression results and comment.

Regression model in e):

```{r}
lm(totalcost ~ comp_bin + ERvisits, data = heart_data_trans) %>%
  summary()
```

```{r}
lm(totalcost ~ comp_bin + ERvisits + age + gender + duration, data = heart_data_trans) %>%
  summary()
```

Comment:

The result shows that there is a relationship between Y and the set of covariates.


### ii) 

Compare the SLR and MLR models. Which model would you use to address the investigator’s objective and why?

Hypothese:

$$
\begin{aligned}
&Model\ 1:\ Y_{i}=\beta_{0} \ +\ \beta_{1}X_{i\ ERvisits} \  +\ \varepsilon_{i}\\
&Model\ 2:\ Y_{i}=\beta_{0} \ +\ \beta_{1}X_{i\ ERvisits} \ +\ \beta_{2}X_{i\ comp\_bin} \ +\ \beta_{3}X_{i\ gender} \ +\ \beta_{4} X_{i\ age} \ +\ \varepsilon_{i}\\
\end{aligned}
$$

$$
\begin{aligned}
&H_{0} : \beta_{2} = \beta_{3} = \beta_{4} = 0\\ 
&H_{1} : at\ least\ one\ \beta\ not\ equal\ to\ zero
\end{aligned}
$$

Decision rule:

$$
F^{*} = \frac{(SSE_{S}-SSE_{L})/(df_{L}-df_{S})}{\frac{SSE_{}}{df_{L}}} \sim F_{df_{L}-df_{S},\ df_{L}}
$$
$$
df_{S}=n-p_{S}-1,\ df_{L}=n-p_{L}-1
$$

$$
\begin{aligned}
&F^{*}=44.49\\
\end{aligned}
$$

$$
\begin{aligned}
&Pr(F>F^{*})< 0.05,\ reject\ H_{0}\\
&Pr(F>F^{*})\geq 0.05,\ fail\ to\ reject\ H_{0}
\end{aligned}
$$

$$
\begin{aligned}
&Pr(F>F^{*})=2.2e-16< 0.05,\ reject\ H_{0}\\
\end{aligned}
$$

```{r}
fit_SLR = lm(totalcost ~ ERvisits, data = heart_data_trans)
fit_MLR = lm(totalcost ~ ERvisits + comp_bin + age + gender + duration, data = heart_data_trans)
anova(fit_SLR, fit_MLR)
```

Conclusion:

Given the result of anova test, it's obviously that the MLR is prefered. So, I would choose MLR to address the investigator's objective.


# Problem 3

A hospital administrator wishes to test the relationship between ‘patient’s satisfaction’ (Y) and ‘age’, ‘severity of illness’, and ‘anxiety level’ (data ‘PatSatisfaction.xlsx’). The administrator randomly selected 46 patients, collected the data, and asked for your help with the analysis.

```{r}
sat_data = read_excel("./data/PatSatisfaction.xlsx")

colnames(sat_data)[1] <- "satisfaction"

sat_data = sat_data %>%
  janitor::clean_names()
```


## a) 

Create a correlation matrix and interpret your initial findings. 

```{r}
cor(sat_data, method = "pearson")
```

The result is a table containing the correlation coefficients between each variable and the others. We can observe that age, severity and axiety have negative relationship with satisfaction. Among those three variables, age has the strongest negative relationship with satisfaction.

## b) 

Fit a multiple regression model and test whether there is a regression relation. State the
hypotheses, decision rule and conclusion.

To build a multiple regression model, we add age, severity and anxiety as predictors:

$$
Y_{i} = \beta_{0} \ +\ \beta_{1}X_{i\ age} \ +\ \beta_{2}X_{i\ anxiety} \ + \beta_{3}X_{i\ severity} + \varepsilon_{i}
$$

Hypotheses:

$$
\begin{aligned}
&H_{0}:\ \beta_{1} = \beta_{2} = \beta_{3}=0\\
&H_{1}:\ at\ least\ one\ \beta\ is \ not\ zero
\end{aligned}
$$

Decision rule:

$$
\begin{aligned}
&Test\ statistic:\\ 
&F^{*} = \frac{MSR}{MSE} > F(1-\alpha;p,n-p-1),\ reject\ H_{0}.\\
\\
&The\ null\ model\ contains\ only\ the\ intercept:\\
&F^{*} = \frac{MSR}{MSE} \leqslant F(1-\alpha;p,n-p-1),\ fail\ to\ reject\ H_{0}
\end{aligned}
$$

$$
\begin{aligned}
&F^{*} = 30.05\\
\end{aligned}
$$

$$
\begin{aligned}
&Pr(F>F^{*})< 0.05,\ reject\ H_{0}\\
&Pr(F>F^{*})\geq 0.05,\ fail\ to\ reject\ H_{0}
\end{aligned}
$$

$$
\begin{aligned}
&Pr(F>F^{*})=1.542e-10 < 0.05,\ reject\ H_{0}\\
\end{aligned}
$$

```{r}
sat_fit = lm(satisfaction ~ age + severity + anxiety, data = sat_data)
summary(sat_fit)
```


Judging from the p-value, we reject the null, which means there is a relationship between predictors and outcome.  


## c) 

Show the regression results for all estimated coefficients with 95% CIs. Interpret the coefficient
and 95% CI associated with ‘severity of illness’.

CI:

$$
\begin{aligned}
&E(Y_{h})=\beta_{0}+\beta_{1}X_{h}\\
&\hat{Y_{h}}=\hat{\beta_{0}}+\hat{\beta_{1}}X_{h}\\
&\hat{\beta_{0}}+\hat{\beta_{1}}X_{h}\pm t_{n-2,1-\alpha/2}\times se(\hat{\beta_{0}}+\hat{\beta_{1}}X_{h})\\
&se(\hat{\beta_{0}}+\hat{\beta_{1}}X_{h})=\sqrt{MSE\left \{\frac{1}{n}+[(X_{h}-\bar{X})^2/\sum_{i=1}^{n}(X_{i}-\bar{X})^2]  \right \}}
\end{aligned}
$$


```{r}
sat_fit = lm(satisfaction ~ age + severity + anxiety, data = sat_data)
summary(sat_fit)
```

```{r}
confint(sat_fit)
```

Interpretation:

The coefficient associated with `severity` means that while other predictors hold constant the satisfaction will decrease 0.44 for each additional unit of the severity of the illness.

The function of `confint` shows the CIs for each estimated coefficients. From the result we can conclude that we are 95% confidence that the mean satisfaction increases by somewhere between -1.434831 and 0.5508228 for each additional unit of the severity of the illness as other predictors hold constant.


## d) 

Obtain an interval estimate for a new patient’s satisfaction when Age=35, Severity=42,
Anxiety=2.1. Interpret the interval.

For a given value of x, the interval estimate of the dependent variable y is called the prediction interval.

PI:

$$
\begin{aligned}
&\hat{\beta_{0}}+\hat{\beta_{1}}X_{h}\pm t_{n-2,1-\alpha/2}\times se(\hat{\beta_{0}}+\hat{\beta_{1}}X_{h})\\
&se(\hat{\beta_{0}}+\hat{\beta_{1}}X_{h})=\sqrt{MSE\left \{\frac{1}{n}+[(X_{h}-\bar{X})^2/\sum_{i=1}^{n}(X_{i}-\bar{X})^2]+1  \right \}}
\end{aligned}
$$

```{r}
pi_data = data.frame(age = 35, severity = 42, anxiety = 2.1)
```

```{r}
predict(sat_fit, pi_data, interval="predict") 
```

The 95% prediction interval of the satisfaction for the age is 35, severity is 42 and anxiety is 2.1 is between 50.06237 and 93.30426. The result means that the probability is 0.95 that this prediction interval will give a correct prediction for the satisfaction when age is 35, severity is 42 and anxiety is 2.1.


## e) 

Test whether ‘anxiety level’ can be dropped from the regression model, given the other two
covariates are retained. State the hypotheses, decision rule and conclusion.

Hypothese:

$$
\begin{aligned}
&Model\ 1:\ Y_{i}=\beta_{0} \ +\ \beta_{1}X_{i\ age} \ +\ \beta_{2}X_{i\ severity} \ +\  \varepsilon_{i}\\
&Model\ 2:\ Y_{i}=\beta_{0} \ +\ \beta_{1}X_{i\ age} \ +\ \beta_{2}X_{i\ severity} \ +\ \beta_{3}X_{i\ anxiety} \ +\  \varepsilon_{i}\\
\end{aligned}
$$

$$
\begin{aligned}
&H_{0} : \beta_{3} = 0\\
&H_{1} : \beta_{3} \neq 0
\end{aligned}
$$

Decision rule:

$$
F^{*} = \frac{(SSE_{S}-SSE_{L})/(df_{L}-df_{S})}{\frac{SSE_{L}}{df_{L}}} \sim F_{df_{L}-df_{S},\ df_{L}}
$$
$$
df_{S}=n-p_{S}-1,\ df_{L}=n-p_{L}-1
$$

$$
\begin{aligned}
&F^{*}=3.5997\\
\end{aligned}
$$

$$
\begin{aligned}
&Pr(F>F^{*})< 0.05,\ reject\ H_{0}\\
&Pr(F>F^{*})\geq 0.05,\ fail\ to\ reject\ H_{0}
\end{aligned}
$$

$$
\begin{aligned}
&Pr(F>F^{*})=0.06468 > 0.05,\ fail\ to\ reject\ H_{0}\\
\end{aligned}
$$

```{r}
anova(lm(satisfaction ~ age + severity, data = sat_data), 
      lm(satisfaction ~ age + severity + anxiety, data = sat_data))
```

Conclusion:

The result shows that the anxiety should NOT be included in the model due to the large p-value at 0.05 significance level and the desicion rule. So we tend to use the smaller model.

