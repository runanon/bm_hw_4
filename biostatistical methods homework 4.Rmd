---
title: "biostatistical methods homework 4"
output: pdf_document
---

```{r message=FALSE}
library(tidyverse)
library(knitr)
library(patchwork)
library(readxl)
```

# Problem1

## (a)

$$
\begin{aligned}
b_{1} &= \frac{n\sum X_{i}Y_{i}-\sum X_{i}\sum Y_{i}}{n\sum X_{i}^{2}-(\sum X_{i})^{2}} \\
&= \frac{\sum X_{i}Y_{i}-n\bar{Y}\bar{X}}{\sum X_{i}^{2}-n\bar{X}^{2}} \\
\\
b_{0} &= \bar{Y}-b_{1}\bar{X}
\\
\\
\sum X_{i}Y_{i}-n\bar{Y}\bar{X} &= \sum X_{i}Y_{i} - \bar{X}\sum{Y_{i}} \\
&= \sum (X_{i}-\bar{X})Y_{i}\\
\\
E\left \{\sum (X_{i} - \bar{X})Y_{i}  \right \} &= \sum(X_{i}-\bar{X})E(Y_{i})\\
&= \sum (X_{i}-\bar{X})(\beta_{0}+\beta_{1}X_{i})\\
&= \beta_{0}\sum X_{i} - n\bar{X}\beta_{0} + \beta_{1}\sum X_{i}^{2} - n\bar{X}^{2}\beta_{1} \\
&= \beta_{1}(\sum X_{i}^2 - n\bar{X}^2)\\
\\
E(b_{1}) &= \frac{E\left \{\sum (X_{i}-\bar{X})Y_{i}  \right \}}{\sum X_{i}^{2}-n\bar{X}^2}\\
&= \frac{\beta_{1}(\sum X_{i}^2 - n\bar{X}^2)}{\sum X_{i}^2 - n\bar{X}^2}\\
&= \beta_{1}\\
\\
E(b_{0}) &= E(\bar{Y}-b_{1}\bar{X})\\
&= \frac{1}{n}\sum E(Y_{i}) - E(b_{1})\bar{X}\\
&= \frac{1}{n}\sum \left [\beta_{0} + \beta_{1}X_{i}  \right ]-\beta_{1}\bar{X}\\
&= \frac{1}{n}\left [n\beta_{0}+n\beta_{1}\bar{X}  \right ]- \beta_{1}\bar{X}\\
&= \beta_{0}
\end{aligned}
$$

## (b)

$$
\begin{aligned}
Y_{i} &= \hat{\beta_{1}}X_{i} + \hat{\beta_{0}}\\
&= \hat{\beta_{1}}X_{i} + \bar{Y} - \hat{\beta_{1}}\bar{X}\\
\\
X_i\ &= \bar{X}\\
Y_{i} &= \hat{\beta_{1}}\bar{X} + \bar{Y} - \hat{\beta_{1}}\bar{X}\\
&=\bar{Y}
\end{aligned}
$$

So the Least Square line equation always goes through the point $(\bar{X}, \bar{Y})$

## (c)
$$
\begin{aligned}
&log_{e}L = -\frac{n}{2}log_{e}2\pi - \frac{n}{2}log_{e}\sigma^{2}-\frac{1}{2\sigma^{2}}\sum (Y_{i}-\beta_{0}-\beta_{1}X_{i})^{2}\\
&\frac{\partial(log_{e}L)}{\partial\sigma^{2}} = -\frac{n}{2\sigma^{2}} + \frac{1}{2\sigma^{4}}\sum (Y_{i}-\beta_{0}-\beta_{1}X_{i})^{2}\\
\end{aligned}
$$

$$
\begin{aligned}
\hat{\sigma}^{2}&=\frac{\sum (Y_{i}-\hat{\beta_{0}}-\hat{\beta_{1}}X_{i})^{2}}{n}\\
&=\frac{\sum(Y_{i}-\hat{Y_{i}})^{2}}{n}
\end{aligned}
$$

### Find its expected value

$$
\begin{aligned}
E(\hat{\sigma}^{2}) &= E\left (\frac{SSE}{n}  \right )\\
&=E\left (\frac{SSE}{n-2}\times \frac{n-2}{n}  \right )\\
&=\frac{n-2}{n} \times E\left (\frac{SSE}{n-2}  \right )\\
&=\frac{n-2}{n}\sigma^{2}
\end{aligned}
$$

### Comment on the unbiasness property

As the result shown above, $\hat{\sigma}^{2}$ is a biasd estimator of $\sigma^{2}$ as the unbiased estimator of $\sigma^{2}$ is MSE:

$$
\begin{aligned}
s^{2} = MSE = \frac{SSE}{n-2} &=  \frac{\sum(Y_{i}-\hat{Y_{i}})^{2}}{n-2} = \frac{\sum e_{i}^2}{n-2}\\
\\
E\left \{MSE  \right \} &= \sigma^{2}
\end{aligned}
$$

# Problem 2

For this problem, you will be using data ‘HeartDisease.csv’. 

```{r message=FALSE}
heart_data = read_csv("./data/HeartDisease.csv")
```


The investigator is mainly interested if there is an association between ‘total cost’ (in dollars) of patients diagnosed with heart disease and the ‘number of emergency room (ER) visits’. 

Further, the model will need to be adjusted for other factors, including ‘age’, ‘gender’, ‘number of complications’ that arose during treatment, and ‘duration of treatment condition’.

## a) 

Provide a short description of the data set: what is the main outcome, main predictor and other important covariates. 

This dataset include `r ncol(heart_data)` variables and `r nrow(heart_data)` observations. The main outcome is `totalcost` which represents the total cost (in dollars) of heart-diseased patients. The main predictor is the `ERvisits` which represents the number of emergency room (ER) visits. Other important covariates are `age`, `gender`, `complications` and `duration`. 

Also, generate appropriate descriptive statistics for all variables of interest (continuous and categorical) – no test required. 

```{r}
mean_and_sd = function(x) {
  
  if (!is.numeric(x)) {
    stop("Argument x should be numeric")
  } else if (length(x) == 1) {
    stop("Cannot be computed for length 1 vectors")
  }
  
  mean_x = mean(x)
  sd_x = sd(x)

  tibble(
    mean = mean_x, 
    sd = sd_x
  )
}
```

### totalcost
```{r}
mean_and_sd(heart_data$totalcost)
```

The mean of the total cost is about 2800 with a standard deviation of 6690.26.

### ERvisits
```{r}
summary(heart_data$ERvisits)
```

The minimum number of emergency room (ER) visits is 0 and the maximum is 20. The median is 3 with 1st Qu. of 2 and 3rd Qu. of 5. 

### age
```{r}
mean_and_sd(heart_data$age)
```

The distribution of age is centered at about 59 with a standard deviation of 6.75.

### gender
```{r}
(summary(as.factor(heart_data$gender)))
```

As 0 represents female and 1 represents male, there are 608 female and 180 male in the dataset.

### complications
```{r}
(summary(as.factor(heart_data$complications)))
```

As we observed from the dataset, there number of complications existing in this dataset is simply 0, 1 and 3. Using summary function, we can conclude that there are 745 patients have zero complicatoins and 42 patients have one complicatoins, and there is only 1 patient has 3 complicaton.

### duration
```{r}
mean_and_sd(heart_data$duration)
```

The average duration of treatment condition is 164 with a standard deviation of 121.

## b) 


```{r}
totalcost_non = heart_data %>%
  ggplot(aes(x = totalcost)) +
  geom_density() + 
  labs(
       x = 'Total Cost',
       y = 'Density'
       )
```

```{r}
totalcost_sq = heart_data %>%
  ggplot(aes(x = (totalcost)^2)) +
  geom_density() +
  labs(
       x = 'Square Transformation of Total Cost',
       y = 'Density'
       ) 
  
```

```{r}
totalcost_log = heart_data %>%
  ggplot(aes(x = log(totalcost))) +
  geom_density() + 
  labs(
       x = 'Log Transformation of Total Cost',
       y = 'Density'
       ) 
```

```{r}
totalcost_sqrt = heart_data %>%
  ggplot(aes(x = sqrt(totalcost))) +
  geom_density() +
  labs(
      x = 'Square Root Transformation of Total Cost',
      y = 'Density'
      )
```

```{r}
(totalcost_non + totalcost_sq) / (totalcost_log + totalcost_sqrt)
```

The shape of the distribution for `totalcost` is right skewed. After trying different transformation we find that the log transformation makes the plot approximate to normal distribution.

## c) 

Create a new variable called ‘comp_bin’ by dichotomizing ‘complications’: 0 if no complications, and 1 otherwise.

```{r}
heart_data = heart_data %>%
  mutate(comp_bin = ifelse(complications == 0, 0, 1)) %>%
  mutate(comp_bin = as.character(comp_bin))
```


## d) 

Based on our decision in part b), fit a simple linear regression (SLR) between the original or transformed ‘total cost’ and predictor ‘ERvisits’. This includes a scatterplot and results of the regression, with appropriate comments on significance and interpretation of the slope.

```{r}
heart_data_trans = heart_data %>%
  mutate(totalcost = log(totalcost)) %>%
  filter(totalcost != -Inf)
```


```{r}
heart_data_trans %>%
  ggplot(aes(x = ERvisits, y = totalcost)) +
  geom_point(color = 'blue') +
  geom_smooth(method = "lm", color = 'red', se = FALSE) +
  labs(
      x = 'The number of ER visits',
      y = 'Log Transformation of Total Cost'
      )
```


```{r}
fit_SLR = lm(totalcost ~ ERvisits, data = heart_data_trans)
summary(fit_SLR)
```

The plot above shows the scatterplot and results of the regression. Using `summary` function, we can see that the estimate slope is 0.22672 with a p-value <2e-16, which strongly indicates that the slope is not equal to 0 and there is significant relationship with `ERvisits` and `totalcost`. The estimate of slope means that when the number of ER visits increase 1, total cost will increase 25%.
$$
\begin{aligned}
log\left (\frac{Y_{2}}{Y_{1}}  \right ) &= \beta_{1} = 0.22672\\
\frac{Y_{2}}{Y_{1}} &= e^{0.22672} = 1.25\\
Y_{2} &= 1.25Y_{1}
\end{aligned}
$$

## e) 

Fit a multiple linear regression (MLR) with ‘comp_bin’ and ‘ERvisits’ as predictors.


### i) 

Test if ‘comp_bin’ is an effect modifier of the relationship between ‘total cost’ and ‘ERvisits’. Comment. 

```{r}
fit_MLR_interaction = lm(totalcost ~ comp_bin * ERvisits, data = heart_data_trans)
summary(fit_MLR_interaction)

```

The definition of modifier is when the magnitude of association differs at different levels of another variable (in this case `comp_bin`), it suggests that effect modification is present. From the result shown above, `comp_bin` is not a modifier according to the p-value of comp_bin1:ERvisits is larger than 0.05.

### ii)

Test if ‘comp_bin’ is a confounder of the relationship between ‘total cost’ and ‘ERvisits’.
Comment. 

```{r}
lm(totalcost ~ comp_bin + ERvisits, data = heart_data_trans) %>%
  summary()

```

Using the summary function we can observe that after adding `comp_bin` as predictor the adjusted R-squared is increasing comparing with only using `ERvisits` as predictor. So `comp_bin` is a confounder. 

### iii) 

Decide if ‘comp_bin’ should be included along with ‘ERvisits'. Why or why not?

`comp_bin` should be included along with `ERvisits` according to the test in ii). The p-value of comp_bin coefficient shows significance. Besides, judging from the adjusted R-squared, when including `comp_bin` the value increases comparing with only using `ERvisits` as predictor. So, `comp_bin` should be included along with `ERvisits`

## f) 

Use your choice of model in part e) and add additional covariates (age, gender, and duration of treatment).

### i) 

Fit a MLR, show the regression results and comment. (5p)

Regression model in e):
```{r}
lm(totalcost ~ comp_bin + ERvisits, data = heart_data_trans) %>%
  summary()
```

Add `age` as a covariate:
```{r}
lm(totalcost ~ comp_bin + ERvisits + age, data = heart_data_trans) %>%
  summary()
```
Judging from p-value and adjusted R-squared, `age` should NOT be included in the model.

Add `gender` as a covariate:
```{r}
heart_data_trans = heart_data_trans %>%
  mutate(gender = as.character(gender))

lm(totalcost ~ comp_bin + ERvisits + gender, data = heart_data_trans) %>%
  summary()
```
Judging from p-value and adjusted R-squared, `gender` should NOT be included in the model.

Add `duration` as a covariate:
```{r}
lm(totalcost ~ comp_bin + ERvisits + duration, data = heart_data_trans) %>%
  summary()
```
Judging from p-value and adjusted R-squared, `duration` should be included in the model.

Test whether `duration` is an effect modifier:
```{r}
lm(totalcost ~ comp_bin * duration + ERvisits, data = heart_data_trans) %>%
  summary()

lm(totalcost ~ comp_bin + ERvisits * duration, data = heart_data_trans) %>%
  summary()

lm(totalcost ~ comp_bin * duration + ERvisits * duration, data = heart_data_trans) %>%
  summary()
```
Judging from the p-value and adjusted R-squared, `duration` is NOT an effective modifier.


In conclusion, the final MLR will include `comp_bin`, `duration` and `ERvisits`:
$$
\hat{Y_{i}} = 1.53X_{i\ comp\_bin} \ +\ 0.17X_{i\ ERvisits} \ +\ 0.005X_{i\ duration} \ +\ 4.76
$$

### ii) 
Compare the SLR and MLR models. Which model would you use to address the investigator’s objective and why? (2p)

```{r}
fit_SLR = lm(totalcost ~ ERvisits, data = heart_data_trans)
fit_MLR = lm(totalcost ~ comp_bin + ERvisits + duration, data = heart_data_trans)
anova(fit_SLR, fit_MLR)
```

Given the result of anova test, it's obviously that the MLR is prefered. So, I would choose MLR to address the investigator's objective.


# Problem 3 (15p)

A hospital administrator wishes to test the relationship between ‘patient’s satisfaction’ (Y) and ‘age’, ‘severity of illness’, and ‘anxiety level’ (data ‘PatSatisfaction.xlsx’). The administrator randomly selected 46 patients, collected the data, and asked for your help with the analysis.

```{r}
sat_data = read_excel("./data/PatSatisfaction.xlsx")

colnames(sat_data)[1] <- "satisfaction"

sat_data = sat_data %>%
  janitor::clean_names()
```


## a) 

Create a correlation matrix and interpret your initial findings. 

```{r}
cor(sat_data, method = "pearson")
```

The result is a table containing the correlation coefficients between each variable and the others. We can observe that age, severity and axiety have negative relationship with satisfaction. Among those three variables, age has the strongest negative relationship with satisfaction.

## b) 

Fit a multiple regression model and test whether there is a regression relation. State the
hypotheses, decision rule and conclusion.

To build a multiple regression model, we add age, severity and anxiety as predictors:
$$
Y_{i} = \beta_{0} \ +\ \beta_{1}X_{i\ age} \ +\ \beta_{2}X_{i\ anxiety} \ + \beta_{3}X_{i\ severity} + \varepsilon_{i}
$$
Hypotheses:
$$
\begin{aligned}
&H_{0}:\ \beta_{1} = \beta_{2} = \beta_{3}=0\\
&H_{1}:\ at\ least\ one\ \beta\ is \ not\ zero
\end{aligned}
$$
Decision rule:
$$
\begin{aligned}
&Test\ statistic:\\ 
&F^{*} = \frac{MSR}{MSE} > F(1-\alpha;p,n-p-1),\ reject\ H_{0}.\\
\\
&The\ null\ model\ contains\ only\ the\ intercept:\\
&F^{*} = \frac{MSR}{MSE} \leqslant F(1-\alpha;p,n-p-1),\ fail\ to\ reject\ H_{0}
\end{aligned}
$$

```{r}
sat_fit = lm(satisfaction ~ age + severity + anxiety, data = sat_data)
summary(sat_fit)

```


```{r}
qf(0.975, 3, 42)
```

$$
\begin{aligned}
&F^{*} = 30.05\\
&F(0.975,\ 3,\ 42)= 3.445689\\
&F^{*}>F(0.975,\ 3,\ 42)
\end{aligned}
$$

Judging from the p-value and F*, we reject the null. 


## c) 

Show the regression results for all estimated coefficients with 95% CIs. Interpret the coefficient
and 95% CI associated with ‘severity of illness’.

CI:
```{r}
sat_fit = lm(satisfaction ~ age + severity + anxiety, data = sat_data)
summary(sat_fit)
```

```{r}
confint(sat_fit)
```

Interpretation:

From the summary we see that the estimated coefficients of age, severity and anxiety is -1.1416, -0.4420 and -13.4702 respectively. Judging from the p-value, only the coefficients of age shows significance. 

The function of `confint` shows the CIs for each estimated coefficients. From the result we can conclude that we are 95% confidence that the true coefficient of severity will fall within the interval of (-1.434831, 0.5508228).


## d) 

Obtain an interval estimate for a new patient’s satisfaction when Age=35, Severity=42,
Anxiety=2.1. Interpret the interval.


For a given value of x, the interval estimate of the dependent variable y is called the prediction interval.

```{r}
pi_data = data.frame(age = 35, severity = 42, anxiety = 2.1)
```

```{r}
predict(sat_fit, pi_data, interval="predict") 
```

The 95% prediction interval of the satisfaction for the age is 35, severity is 42 and anxiety is 2.1 is between 50.06237 and 93.30426. The result means that we are 95% confidence that a future observation of satisfaction for age is 35, severity is 42 and anxiety is 2.1 will be contained within the interval between 50.06237 and 93.30426.


## e) 

Test whether ‘anxiety level’ can be dropped from the regression model, given the other two
covariates are retained. State the hypotheses, decision rule and conclusion. (3p)

The hypothese are as below:
$$
\begin{aligned}
&Model\ 1:\ Y_{i}=\beta_{0} \ +\ \beta_{1}X_{i\ age} \ +\ \beta_{2}X_{i\ severity} \ +\  \varepsilon_{i}\\
&Model\ 2:\ Y_{i}=\beta_{0} \ +\ \beta_{1}X_{i\ age} \ +\ \beta_{2}X_{i\ severity} \ +\ \beta_{3}X_{i\ anxiety} \ +\  \varepsilon_{i}\\
\end{aligned}
$$

$$
\begin{aligned}
&H_{0} : \beta_{3} = 0\\
&H_{1} : \beta_{3} \neq 0
\end{aligned}
$$

```{r}
anova(lm(satisfaction ~ age + severity, data = sat_data), 
      lm(satisfaction ~ age + severity + anxiety, data = sat_data))
```

The result shows that the anxiety should NOT be included in the model.



